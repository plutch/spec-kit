---
description: Execute the implementation plan by processing and executing all tasks defined in tasks.md
scripts:
  sh: .specify/scripts/bash/check-prerequisites.sh --json --require-tasks --include-tasks
  ps: .specify/scripts/powershell/check-prerequisites.ps1 -Json -RequireTasks -IncludeTasks
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

1. Run `{SCRIPT}` from repo root and parse FEATURE_DIR and AVAILABLE_DOCS list. All paths must be absolute. For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

2. **Pre-Implementation Approval Gate Check** (NEW v2.3):

   **Check if previous phases are approved before proceeding:**

   - Read `spec-metadata.json` from FEATURE_DIR (if exists)
   - Check approval status and current phase

   **Approval Gate Logic**:
   ```
   IF spec-metadata.json exists:
     IF approvals.planning.approved == false:
       â†’ ðŸ”´ ERROR: "Planning not approved. Review plan.md and approve before implementation."
       â†’ STOP execution

     IF approvals.tasks.approved == false AND tasks.md exists:
       â†’ ðŸŸ  WARN: "Tasks not approved. Recommend reviewing tasks.md before proceeding."
       â†’ Ask: "Tasks not approved. Proceed anyway? (yes/no/-y for auto-approve)"
       â†’ IF no: STOP
       â†’ IF yes or -y flag: Continue (update tasks.approved = true)

     IF approvals.gap_analysis.approved == false AND metadata.risk_level == "HIGH":
       â†’ ðŸŸ  WARN: "HIGH RISK feature without gap analysis approval."
       â†’ Recommend: "Run /speckit.status to review workflow state"

   ELSE:
     â†’ â„¹ï¸ INFO: "No spec-metadata.json found - proceeding without approval gates"
     â†’ Recommend: "Consider running /speckit.memory to set up project memory for better workflow tracking"
   ```

   **Auto-Approval Flag**: If user provides `-y` flag, automatically approve current phase and proceed.

3. **Check checklists status** (if FEATURE_DIR/checklists/ exists):
   - Scan all checklist files in the checklists/ directory
   - For each checklist, count:
     - Total items: All lines matching `- [ ]` or `- [X]` or `- [x]`
     - Completed items: Lines matching `- [X]` or `- [x]`
     - Incomplete items: Lines matching `- [ ]`
   - Create a status table:

     ```text
     | Checklist | Total | Completed | Incomplete | Status |
     |-----------|-------|-----------|------------|--------|
     | ux.md     | 12    | 12        | 0          | âœ“ PASS |
     | test.md   | 8     | 5         | 3          | âœ— FAIL |
     | security.md | 6   | 6         | 0          | âœ“ PASS |
     ```

   - Calculate overall status:
     - **PASS**: All checklists have 0 incomplete items
     - **FAIL**: One or more checklists have incomplete items

   - **If any checklist is incomplete**:
     - Display the table with incomplete item counts
     - **STOP** and ask: "Some checklists are incomplete. Do you want to proceed with implementation anyway? (yes/no)"
     - Wait for user response before continuing
     - If user says "no" or "wait" or "stop", halt execution
     - If user says "yes" or "proceed" or "continue", proceed to step 3

   - **If all checklists are complete**:
     - Display the table showing all checklists passed
     - Automatically proceed to step 3

3. Load and analyze the implementation context:
   - **REQUIRED**: Read tasks.md for the complete task list and execution plan
   - **REQUIRED**: Read plan.md for tech stack, architecture, and file structure
   - **IF EXISTS**: Read data-model.md for entities and relationships
   - **IF EXISTS**: Read contracts/ for API specifications and test requirements
   - **IF EXISTS**: Read research.md for technical decisions and constraints
   - **IF EXISTS**: Read quickstart.md for integration scenarios

4. **Project Setup Verification**:
   - **REQUIRED**: Create/verify ignore files based on actual project setup:

   **Detection & Creation Logic**:
   - Check if the following command succeeds to determine if the repository is a git repo (create/verify .gitignore if so):

     ```sh
     git rev-parse --git-dir 2>/dev/null
     ```

   - Check if Dockerfile* exists or Docker in plan.md â†’ create/verify .dockerignore
   - Check if .eslintrc*or eslint.config.* exists â†’ create/verify .eslintignore
   - Check if .prettierrc* exists â†’ create/verify .prettierignore
   - Check if .npmrc or package.json exists â†’ create/verify .npmignore (if publishing)
   - Check if terraform files (*.tf) exist â†’ create/verify .terraformignore
   - Check if .helmignore needed (helm charts present) â†’ create/verify .helmignore

   **If ignore file already exists**: Verify it contains essential patterns, append missing critical patterns only
   **If ignore file missing**: Create with full pattern set for detected technology

   **Common Patterns by Technology** (from plan.md tech stack):
   - **Node.js/JavaScript/TypeScript**: `node_modules/`, `dist/`, `build/`, `*.log`, `.env*`
   - **Python**: `__pycache__/`, `*.pyc`, `.venv/`, `venv/`, `dist/`, `*.egg-info/`
   - **Java**: `target/`, `*.class`, `*.jar`, `.gradle/`, `build/`
   - **C#/.NET**: `bin/`, `obj/`, `*.user`, `*.suo`, `packages/`
   - **Go**: `*.exe`, `*.test`, `vendor/`, `*.out`
   - **Ruby**: `.bundle/`, `log/`, `tmp/`, `*.gem`, `vendor/bundle/`
   - **PHP**: `vendor/`, `*.log`, `*.cache`, `*.env`
   - **Rust**: `target/`, `debug/`, `release/`, `*.rs.bk`, `*.rlib`, `*.prof*`, `.idea/`, `*.log`, `.env*`
   - **Kotlin**: `build/`, `out/`, `.gradle/`, `.idea/`, `*.class`, `*.jar`, `*.iml`, `*.log`, `.env*`
   - **C++**: `build/`, `bin/`, `obj/`, `out/`, `*.o`, `*.so`, `*.a`, `*.exe`, `*.dll`, `.idea/`, `*.log`, `.env*`
   - **C**: `build/`, `bin/`, `obj/`, `out/`, `*.o`, `*.a`, `*.so`, `*.exe`, `Makefile`, `config.log`, `.idea/`, `*.log`, `.env*`
   - **Swift**: `.build/`, `DerivedData/`, `*.swiftpm/`, `Packages/`
   - **R**: `.Rproj.user/`, `.Rhistory`, `.RData`, `.Ruserdata`, `*.Rproj`, `packrat/`, `renv/`
   - **Universal**: `.DS_Store`, `Thumbs.db`, `*.tmp`, `*.swp`, `.vscode/`, `.idea/`

   **Tool-Specific Patterns**:
   - **Docker**: `node_modules/`, `.git/`, `Dockerfile*`, `.dockerignore`, `*.log*`, `.env*`, `coverage/`
   - **ESLint**: `node_modules/`, `dist/`, `build/`, `coverage/`, `*.min.js`
   - **Prettier**: `node_modules/`, `dist/`, `build/`, `coverage/`, `package-lock.json`, `yarn.lock`, `pnpm-lock.yaml`
   - **Terraform**: `.terraform/`, `*.tfstate*`, `*.tfvars`, `.terraform.lock.hcl`
   - **Kubernetes/k8s**: `*.secret.yaml`, `secrets/`, `.kube/`, `kubeconfig*`, `*.key`, `*.crt`

5. Parse tasks.md structure and extract:
   - **Task phases**: Setup, Tests, Core, Integration, Polish
   - **Task dependencies**: Sequential vs parallel execution rules
   - **Task details**: ID, description, file paths, parallel markers [P]
   - **Execution flow**: Order and dependency requirements

6. Execute implementation following the task plan:
   - **Continuous execution**: Execute ALL tasks across all phases without interruption until complete
   - **Respect dependencies**: Run sequential tasks in order, parallel tasks [P] can run together
   - **Follow TDD approach**: Execute test tasks before their corresponding implementation tasks
   - **File-based coordination**: Tasks affecting the same files must run sequentially
   - **No phase checkpoints**: Validation occurs AFTER all tasks complete via sequential reviewers

7. Implementation execution rules:
   - **Setup first**: Initialize project structure, dependencies, configuration
   - **Tests before code**: If you need to write tests for contracts, entities, and integration scenarios
   - **Core development**: Implement models, services, CLI commands, endpoints
   - **Integration work**: Database connections, middleware, logging, external services
   - **Polish and validation**: Unit tests, performance optimization, documentation

8. Progress tracking and error handling:
   - Report progress after each completed task
   - Halt execution if any non-parallel task fails
   - For parallel tasks [P], continue with successful tasks, report failed ones
   - Provide clear error messages with context for debugging
   - Suggest next steps if implementation cannot proceed
   - **IMPORTANT** For completed tasks, make sure to mark the task off as [X] in the tasks file.

9. **Quality Gate Validation** (Parallel Execution):

   **Purpose**: Execute all validation checks in parallel to minimize latency while maintaining comprehensive quality assurance. All three reviewers (Code, Quality/Tests, Security) run concurrently, then results are aggregated.

   **Execution Model**: Run Code, Quality/Tests, and Security reviewers simultaneously (not sequentially), then aggregate results.

   **Performance Benefit**: ~53% faster than sequential execution (95s â†’ 45s for typical projects)

   ---

   ### Parallel Validation Block

   Execute all reviewers simultaneously (not sequentially):

   #### 9.1 Code Quality Reviewer (Parallel Track 1)

   **Validation Tasks** (Run in parallel with other reviewers):

   **9.1.1 Code Quality Validation** ðŸŸ  MAJOR

   - [ ] **Linting**: Run project linter (eslint, pylint, clippy, etc.)
     - Zero errors allowed for critical severity issues
     - Warnings documented or suppressed with justification
     - Command: `npm run lint` / `cargo clippy` / `pylint src/` (project-specific)

   - [ ] **Formatting**: Run code formatter check
     - All files formatted consistently (prettier, black, rustfmt, gofmt)
     - No mixed tabs/spaces, consistent indentation
     - Command: `npm run format:check` / `black --check .` / `cargo fmt --check`

   - [ ] **Type Safety** (if TypeScript/typed language):
     - Zero type errors
     - No `any` types in production code (or explicitly justified)
     - Command: `tsc --noEmit` / `mypy src/` / `flow check`

   - [ ] **Dead Code Detection**:
     - No unused imports or variables
     - No commented-out code blocks (remove or document why kept)
     - No unreachable code paths

   **9.1.2 Specification Alignment** ðŸŸ  MAJOR

   - [ ] **Requirements Traceability**:
     - All P1/P2 User Stories implemented
     - All FR-XXX Functional Requirements addressed
     - All BR-XXX Business Rules enforced in code
     - Spot-check: Pick 3 random FRs, verify implementation matches spec

   - [ ] **Acceptance Criteria Met**:
     - All Given/When/Then scenarios testable
     - Automated tests validate acceptance criteria
     - Manual testing performed for UI/UX scenarios (if applicable)

   - [ ] **Technical Plan Adherence**:
     - File structure matches plan.md architecture
     - Tech stack used as specified (no surprise dependencies)
     - API contracts match contracts/ specifications
     - Data model matches data-model.md

   **9.1.3 Documentation Completeness** ðŸŸ¡ MEDIUM

   - [ ] **Code Documentation**:
     - Public APIs documented (JSDoc, Rustdoc, docstrings)
     - Complex algorithms explained
     - Non-obvious decisions have comments

   - [ ] **README/Quick-start Updated**:
     - Installation steps current
     - Environment variables documented
     - Development setup instructions accurate

   - [ ] **API Documentation** (if applicable):
     - OpenAPI/Swagger spec updated
     - Endpoint descriptions current
     - Example requests/responses provided

   **Code Reviewer Status** (determined after parallel execution):
   - âœ… **READY**: All checks pass, <5 linting warnings, all requirements traced, docs complete
   - âš ï¸ **NEEDS REVIEW**: 5-20 linting warnings, minor spec gaps (<10% requirements), incomplete docs
   - âŒ **NOT READY**: Linting errors, type errors, P1 requirements missing, acceptance criteria unmet

   ---

   #### 9.2 Quality/Tests Reviewer (Parallel Track 2)

   **Validation Tasks** (Run in parallel with other reviewers):

   **9.2.1 Test Execution** ðŸ”´ CRITICAL

   - [ ] **All Tests Pass**:
     - Unit tests passing (Jest, pytest, cargo test, go test)
     - Integration tests passing
     - E2E tests passing (if applicable)
     - No skipped tests without documented justification
     - Command: `npm test` / `cargo test` / `pytest` / `go test ./...`

   **9.2.2 Coverage Thresholds** ðŸŸ  MAJOR

   - [ ] **Coverage Requirements Met** (from Test Strategy in plan.md):
     - **Critical paths**: â‰¥ 90% coverage (high-risk features, security, payments)
     - **Business logic**: â‰¥ 80% coverage (user stories, core workflows)
     - **Overall codebase**: â‰¥ 75% coverage
     - Command: `npm run test:coverage` / `cargo tarpaulin` / `pytest --cov`

   - [ ] **Coverage by Risk Level**:
     - Map coverage to Risk Assessment from spec.md
     - All ðŸ”´ HIGH-RISK requirements (Risk Score â‰¥ 8) must have â‰¥ 90% coverage
     - ðŸŸ  MEDIUM-RISK requirements should have â‰¥ 80% coverage

   **9.2.3 High-Risk Requirements Testing** ðŸŸ  MAJOR

   For features with Risk Score â‰¥ 8 (from spec.md Risk Assessment):

   - [ ] **Edge Case Coverage**:
     - Boundary conditions tested (min/max values, empty inputs, limits)
     - Error conditions tested (timeouts, invalid input, auth failures)
     - State transitions tested (order of operations, concurrent access)
     - Race conditions tested (if multi-threaded/async)

   - [ ] **Production Resilience Tests**:
     - Timeout handling tested (external APIs, database queries)
     - Retry logic tested (exponential backoff, max retries)
     - Idempotency tested (can replay requests safely)
     - Crash recovery tested (graceful degradation)

   **9.2.4 Build Readiness** ðŸ”´ CRITICAL

   - [ ] **Production Build Success**:
     - Build succeeds with zero errors
     - No build warnings in production mode
     - Command: `npm run build` / `cargo build --release` / `go build`

   - [ ] **Docker Build** (if using containers):
     - Dockerfile builds successfully
     - Image size reasonable (< 500MB if possible)
     - Multi-stage builds used (if applicable)
     - Command: `docker build -t feature-test .`

   - [ ] **Environment Configuration**:
     - .env.example updated with new variables
     - All required env vars documented
     - No production secrets in .env.example

   **Quality/Tests Reviewer Status** (determined after parallel execution):
   - âœ… **READY**: All tests pass, coverage thresholds met, high-risk requirements tested, build succeeds
   - âš ï¸ **NEEDS REVIEW**: 70-75% overall coverage (but critical paths â‰¥90%), minor edge cases missing, non-blocking build warnings
   - âŒ **NOT READY**: ANY test failures, coverage <70%, critical paths <90%, build fails

   ---

   #### 9.3 Security Reviewer (Parallel Track 3)

   **Validation Tasks** (Run in parallel with other reviewers):

   **9.3.1 Secrets Scanning** ðŸ”´ CRITICAL

   - [ ] **No Hardcoded Secrets**:
     - Scan for API keys, passwords, tokens, private keys in code
     - All secrets loaded from environment variables or vault
     - No credentials in config files, logs, or comments
     - Commands:
       ```bash
       git secrets --scan
       truffleHog --regex --entropy=False .
       gitleaks detect
       grep -rE "(api_key|password|secret|token|private_key)" src/
       ```

   - [ ] **Secret Management Verification**:
     - .env.example contains placeholder values only (NO real secrets)
     - All required secrets documented in .env.example
     - Secrets loading mechanism implemented (dotenv, vault client)

   **9.3.2 Authentication & Authorization** ðŸ”´ CRITICAL

   - [ ] **Authentication Implementation**:
     - Token validation present (JWT, OAuth, session)
     - Token expiration enforced
     - Refresh token rotation (if applicable)
     - Password hashing (bcrypt, argon2, scrypt - NOT md5/sha1)

   - [ ] **Authorization Controls**:
     - Protected endpoints have auth middleware
     - RBAC/ABAC enforced (role-based / attribute-based access control)
     - Principle of least privilege applied
     - Admin functions have extra protection (2FA, IP whitelist, audit log)

   - [ ] **Database Query Filtering**:
     - All queries filtered by user/tenant ID
     - Row-level security (RLS) enabled (if using Postgres)
     - No data leakage between tenants/users

   **9.3.3 Input Validation & Injection Prevention** ðŸŸ  MAJOR

   - [ ] **Input Validation**:
     - All user inputs validated (API endpoints, forms, URL params)
     - Schema validation implemented (Zod, Joi, JSON Schema, class-validator)
     - Whitelist validation (reject unexpected fields)
     - Length limits enforced (prevent DoS)

   - [ ] **SQL Injection Prevention**:
     - All SQL queries parameterized (prepared statements, ORM)
     - NO string concatenation in SQL queries
     - Verify: `grep -r "SELECT.*+" src/` (should return nothing suspicious)

   - [ ] **XSS Prevention**:
     - HTML output escaped (template engines auto-escape by default)
     - Content Security Policy (CSP) headers set
     - User-generated content sanitized (DOMPurify, sanitize-html)

   - [ ] **Path Traversal Prevention**:
     - File paths sanitized (no `../` in user inputs)
     - File uploads validated (extension, MIME type, size)

   **9.3.4 OWASP Top 10 Validation** ðŸŸ  MAJOR

   Comprehensive checklist from plan.md Security Review (Phase 3):

   - [ ] **A01: Broken Access Control**
     - Authorization checks on all protected resources
     - No horizontal privilege escalation (user A can't access user B's data)
     - No vertical privilege escalation (regular user can't access admin functions)

   - [ ] **A02: Cryptographic Failures**
     - Sensitive data encrypted at rest (PII, credentials)
     - Sensitive data encrypted in transit (HTTPS, TLS 1.2+)
     - No weak encryption (AES-256, RSA-2048+, NOT DES/RC4)

   - [ ] **A03: Injection**
     - SQL injection prevented (parameterized queries)
     - NoSQL injection prevented (sanitized queries)
     - Command injection prevented (no shell execution from user input)
     - LDAP/XML injection prevented (if applicable)

   - [ ] **A04: Insecure Design**
     - Threat modeling performed (from Risk Assessment in spec.md)
     - Rate limiting implemented (prevent brute force, DoS)
     - Business logic abuse prevented (cart manipulation, discount stacking)

   - [ ] **A05: Security Misconfiguration**
     - Default credentials changed
     - Unnecessary features disabled
     - Error messages don't leak sensitive info (stack traces hidden in production)
     - Security headers set (X-Frame-Options, X-Content-Type-Options, HSTS)

   - [ ] **A06: Vulnerable and Outdated Components**
     - Dependencies up-to-date (or documented exceptions)
     - Security audit passed (see 9.3.5 below)

   - [ ] **A07: Identification and Authentication Failures**
     - Weak password policy NOT allowed (minimum length, complexity)
     - Brute force protection (rate limiting, account lockout)
     - Session management secure (HttpOnly, Secure, SameSite cookies)
     - Multi-factor authentication (if required by spec.md)

   - [ ] **A08: Software and Data Integrity Failures**
     - Software updates signed/verified
     - CI/CD pipeline secured (no code injection possible)
     - Deserialization secure (no unsafe pickle/yaml.load)

   - [ ] **A09: Security Logging and Monitoring Failures**
     - Authentication events logged (login, logout, failed attempts)
     - Authorization failures logged (access denied)
     - Data access logged (sensitive operations, admin actions)
     - Logs stored securely (no PII in logs, access controlled)

   - [ ] **A10: Server-Side Request Forgery (SSRF)**
     - URL inputs validated (whitelist allowed domains)
     - Internal services not accessible from user input
     - Redirect validation (no open redirects)

   **9.3.5 Dependency Security Audits** ðŸŸ  MAJOR

   - [ ] **Run Security Audits**:
     - Command: `npm audit --audit-level=high` (Node.js)
     - Command: `cargo audit` (Rust)
     - Command: `pip-audit` (Python)
     - Command: `go list -json -m all | nancy sleuth` (Go)

   - [ ] **Vulnerability Assessment**:
     - **HIGH/CRITICAL**: MUST fix or provide mitigation plan
     - **MEDIUM**: Document and create backlog ticket
     - **LOW**: Document for future fix

   **9.3.6 Risk Mitigation Validation** ðŸ”´ CRITICAL

   For HIGH-risk features (Risk Score 8-12 from spec.md Risk Assessment):

   - [ ] **Risk Mitigation Controls Implemented**:
     - Verify all mitigation strategies from Risk Assessment table
     - Security controls from Security Review (plan.md Phase 3) present
     - Performance requirements met (if specified)

   - [ ] **Compliance Requirements Met** (if applicable):
     - **GDPR**: Right to access, right to delete, consent management
     - **CCPA**: Data disclosure, opt-out mechanisms
     - **PCI-DSS**: No card storage, tokenization, secure transmission
     - **HIPAA**: PHI encryption, access controls, audit logging
     - **SOC 2**: Access controls, logging, data protection

   **Security Reviewer Status** (determined after parallel execution):
   - âœ… **READY**: No secrets, auth implemented, OWASP mitigated, no HIGH/CRITICAL vulnerabilities, risk controls present
   - âš ï¸ **NEEDS REVIEW**: LOW/MEDIUM vulnerabilities documented, partial OWASP coverage (non-critical categories)
   - âŒ **NOT READY**: Hardcoded secrets, missing auth, SQL injection risk, HIGH/CRITICAL vulnerabilities, missing risk controls

   ---

   ### Aggregated Quality Gate Report

   **Purpose**: Combine results from all 3 reviewers into a single comprehensive report.

   **Wait for ALL parallel reviewers to complete**, then aggregate:

   ```markdown
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   ðŸ“‹ Quality Gate Summary (Parallel Validation)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

   **Feature**: [FEATURE_NAME]
   **Date**: [DATE]
   **Execution Time**: [N]s (parallel) vs ~95s (sequential) â†’ [X]% faster

   | Reviewer | Status | Critical Issues | Warnings | Details |
   |----------|--------|----------------|----------|---------|
   | Code Reviewer | [âœ…/âš ï¸/âŒ] | [N] errors | [M] warnings | Linting, type safety, spec alignment, docs |
   | Quality/Tests | [âœ…/âš ï¸/âŒ] | [N] failures | [M] warnings | Test execution, coverage, build readiness |
   | Security | [âœ…/âš ï¸/âŒ] | [N] vulns | [M] low-severity | Secrets, auth, OWASP, dependency audit |

   **Overall Status**: [âœ… READY / âš ï¸ NEEDS REVIEW / âŒ NOT READY]

   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   ## Code Reviewer Findings
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

   | Validation Area | Status | Details |
   |----------------|--------|---------|
   | Linting | [ðŸŸ¢/ðŸŸ¡/ðŸ”´] | [N] errors, [N] warnings |
   | Formatting | [ðŸŸ¢/ðŸ”´] | All files consistent / [N] format violations |
   | Type Safety | [ðŸŸ¢/ðŸ”´] | Zero type errors / [N] type errors |
   | Dead Code | [ðŸŸ¢/ðŸŸ¡] | Clean / [N] unused imports |
   | Requirements Traceability | [ðŸŸ¢/ðŸŸ¡/ðŸ”´] | [N]/[N] requirements traced |
   | Acceptance Criteria | [ðŸŸ¢/ðŸ”´] | All met / [N] unmet |
   | Technical Plan | [ðŸŸ¢/ðŸ”´] | Matches plan / deviations found |
   | Code Documentation | [ðŸŸ¢/ðŸŸ¡] | Complete / [N] functions missing docs |
   | README/API Docs | [ðŸŸ¢/ðŸŸ¡] | Updated / needs sections |

   **Findings**:
   - âœ… [List passed checks - ONLY if exists]
   - âš ï¸  [List warnings - ONLY if exists]
   - ðŸ”´ [List failures - ONLY if exists]

   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   ## Quality/Tests Reviewer Findings
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

   | Validation Area | Status | Details |
   |----------------|--------|---------|
   | Test Execution | [ðŸŸ¢/ðŸ”´] | [N] tests passed, [N] failed |
   | Unit Tests | [ðŸŸ¢/ðŸ”´] | [N]/[N] passed |
   | Integration Tests | [ðŸŸ¢/ðŸ”´] | [N]/[N] passed |
   | E2E Tests | [ðŸŸ¢/ðŸ”´/N/A] | [N]/[N] passed |
   | Coverage - Critical | [ðŸŸ¢/ðŸŸ¡/ðŸ”´] | [N]% (target â‰¥90%) |
   | Coverage - Business | [ðŸŸ¢/ðŸŸ¡/ðŸ”´] | [N]% (target â‰¥80%) |
   | Coverage - Overall | [ðŸŸ¢/ðŸŸ¡/ðŸ”´] | [N]% (target â‰¥75%) |
   | High-Risk Testing | [ðŸŸ¢/ðŸŸ¡/ðŸ”´] | [N]/[N] requirements tested |
   | Edge Cases | [ðŸŸ¢/ðŸŸ¡] | Complete / [N] missing |
   | Production Resilience | [ðŸŸ¢/ðŸŸ¡] | Tested / partial |
   | Build Success | [ðŸŸ¢/ðŸ”´] | Success / [N] errors |
   | Docker Build | [ðŸŸ¢/ðŸ”´/N/A] | Success / failed |
   | Env Configuration | [ðŸŸ¢/ðŸŸ¡] | Complete / needs update |

   **Test Summary**:
   - Total Tests: [N]
   - Passed: [N]
   - Failed: [N]
   - Skipped: [N] (with justification)

   **Coverage Breakdown**:
   - Critical Paths: [N]% ([N]/[N] lines)
   - Business Logic: [N]% ([N]/[N] lines)
   - Overall: [N]% ([N]/[N] lines)

   **High-Risk Requirements** (Risk Score â‰¥ 8):
   | Requirement | Risk Score | Tests | Coverage |
   |-------------|------------|-------|----------|
   | [FR-XXX] | [N] | [N] tests | [N]% |

   **Findings**:
   - âœ… [List passed checks - ONLY if exists]
   - âš ï¸  [List warnings - ONLY if exists]
   - ðŸ”´ [List failures - ONLY if exists]

   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   ## Security Reviewer Findings
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

   | Validation Area | Status | Details |
   |----------------|--------|---------|
   | Secrets Scanning | [ðŸŸ¢/ðŸ”´] | No secrets / [N] secrets found |
   | Authentication | [ðŸŸ¢/ðŸ”´] | Implemented / missing |
   | Authorization | [ðŸŸ¢/ðŸ”´] | RBAC enforced / gaps found |
   | Input Validation | [ðŸŸ¢/ðŸŸ¡/ðŸ”´] | Complete / partial / missing |
   | SQL Injection Prevention | [ðŸŸ¢/ðŸ”´] | Parameterized queries / vulnerable |
   | XSS Prevention | [ðŸŸ¢/ðŸŸ¡/ðŸ”´] | Escaped output / vulnerable |
   | OWASP A01 (Access Control) | [ðŸŸ¢/ðŸŸ¡/ðŸ”´] | Mitigated / partial / vulnerable |
   | OWASP A02 (Cryptography) | [ðŸŸ¢/ðŸŸ¡/ðŸ”´] | Mitigated / partial / vulnerable |
   | OWASP A03 (Injection) | [ðŸŸ¢/ðŸŸ¡/ðŸ”´] | Mitigated / partial / vulnerable |
   | OWASP A04 (Insecure Design) | [ðŸŸ¢/ðŸŸ¡/ðŸ”´] | Mitigated / partial / vulnerable |
   | OWASP A05 (Misconfiguration) | [ðŸŸ¢/ðŸŸ¡/ðŸ”´] | Mitigated / partial / vulnerable |
   | OWASP A06 (Vulnerable Components) | [ðŸŸ¢/ðŸŸ¡/ðŸ”´] | Audited / vulnerabilities found |
   | OWASP A07 (Auth Failures) | [ðŸŸ¢/ðŸŸ¡/ðŸ”´] | Mitigated / partial / vulnerable |
   | OWASP A08 (Integrity Failures) | [ðŸŸ¢/ðŸŸ¡/ðŸ”´] | Mitigated / partial / vulnerable |
   | OWASP A09 (Logging Failures) | [ðŸŸ¢/ðŸŸ¡/ðŸ”´] | Mitigated / partial / vulnerable |
   | OWASP A10 (SSRF) | [ðŸŸ¢/ðŸŸ¡/ðŸ”´] | Mitigated / partial / vulnerable |
   | Dependency Audit | [ðŸŸ¢/ðŸŸ¡/ðŸ”´] | Clean / [N] vulnerabilities |
   | Risk Mitigation | [ðŸŸ¢/ðŸ”´] | Controls implemented / missing |
   | Compliance | [ðŸŸ¢/ðŸŸ¡/N/A] | Requirements met / partial |

   **OWASP Top 10 Summary**:
   - Mitigated: [N]/10 categories
   - Partial: [N]/10 categories
   - Vulnerable: [N]/10 categories

   **Dependency Vulnerabilities**:
   - HIGH/CRITICAL: [N]
   - MEDIUM: [N]
   - LOW: [N]

   **High-Risk Requirements** (Risk Score â‰¥ 8):
   | Requirement | Risk Score | Mitigation Status |
   |-------------|------------|-------------------|
   | [FR-XXX] | [N] | [âœ… Implemented / âš ï¸ Partial / ðŸ”´ Missing] |

   **Findings**:
   - âœ… [List passed checks - ONLY if exists]
   - âš ï¸  [List warnings - ONLY if exists]
   - ðŸ”´ [List failures - ONLY if exists]

   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   ## Overall Decision
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

   **Aggregation Logic Applied**:

   IF Code = âŒ NOT READY OR Quality/Tests = âŒ NOT READY OR Security = âŒ NOT READY:
     â†’ Overall: âŒ NOT READY
     â†’ Message: "Critical issues found. MUST fix before proceeding."
     â†’ Action: STOP workflow, present all failures to user

   ELSE IF Code = âš ï¸ NEEDS REVIEW OR Quality/Tests = âš ï¸ NEEDS REVIEW OR Security = âš ï¸ NEEDS REVIEW:
     â†’ Overall: âš ï¸ NEEDS REVIEW
     â†’ Message: "Minor issues present. Review warnings and decide to proceed or fix."
     â†’ Action: Ask user: "Proceed with warnings or fix issues first?"

   ELSE:
     â†’ Overall: âœ… READY
     â†’ Message: "All quality gates passed. Ready for final validation."
     â†’ Action: Proceed to Step 10 (Final Validation & Completion)

   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   ## Next Action
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

   [IF âœ… READY:]
   âœ… All quality gates passed. Proceeding to Final Validation (Step 10).

   [IF âš ï¸ NEEDS REVIEW:]
   âš ï¸  Minor issues present. Options:
   1. Fix warnings now (recommended for critical features)
   2. Document warnings and proceed
   3. Create backlog tickets for warnings

   User, do you want to proceed or fix warnings first?

   [IF âŒ NOT READY:]
   âŒ CRITICAL ISSUES FOUND - Implementation NOT ready for commit.

   **Required Actions**:
   [List all blocking failures from all 3 reviewers]

   Fix these critical issues, then re-run quality gate validation.

   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   ```

   ---

   ### Overall Status Determination

   **Aggregation Logic** (applied after all parallel reviewers complete):

   ```yaml
   COLLECT reviewer statuses:
     code_status = [Code Reviewer Status: âœ…/âš ï¸/âŒ]
     quality_status = [Quality/Tests Reviewer Status: âœ…/âš ï¸/âŒ]
     security_status = [Security Reviewer Status: âœ…/âš ï¸/âŒ]

   DETERMINE overall status:
     IF code_status = âŒ NOT READY OR quality_status = âŒ NOT READY OR security_status = âŒ NOT READY:
       â†’ Overall: âŒ NOT READY
       â†’ Message: "Critical issues found in [failing reviewers]. MUST fix before proceeding."
       â†’ Action: STOP workflow, present aggregated report with all failures

     ELSE IF code_status = âš ï¸ NEEDS REVIEW OR quality_status = âš ï¸ NEEDS REVIEW OR security_status = âš ï¸ NEEDS REVIEW:
       â†’ Overall: âš ï¸ NEEDS REVIEW
       â†’ Message: "Minor issues present. Review warnings and decide to proceed or fix."
       â†’ Action: Ask user: "Proceed with warnings or fix issues first?"

     ELSE:
       â†’ Overall: âœ… READY
       â†’ Message: "All quality gates passed. Ready for final validation."
       â†’ Action: Proceed to Step 10 (Final Validation & Completion)

   PRESENT aggregated quality gate report to user with overall status.
   ```

   ---

   ### Blocking Criteria (Aggregated from All Reviewers)

   **CANNOT proceed to Step 10 if ANY of the following**:

   **Code Reviewer Blockers**:
   - ðŸ”´ Linting/type errors present
   - ðŸ”´ P1 User Stories not implemented
   - ðŸ”´ Acceptance criteria unmet

   **Quality/Tests Reviewer Blockers**:
   - ðŸ”´ ANY test failures
   - ðŸ”´ Coverage <70% overall
   - ðŸ”´ Critical paths <90% coverage
   - ðŸ”´ High-risk requirements (Score â‰¥8) without tests
   - ðŸ”´ Build fails

   **Security Reviewer Blockers**:
   - ðŸ”´ Hardcoded secrets detected
   - ðŸ”´ Missing authentication on protected endpoints
   - ðŸ”´ SQL injection / XSS vulnerabilities
   - ðŸ”´ HIGH/CRITICAL dependency vulnerabilities
   - ðŸ”´ High-risk requirements (Score â‰¥8) missing mitigation controls
   - ðŸ”´ CRITICAL OWASP categories (A01, A02, A03, A07, A08) vulnerable

   ---

   ### Warning Criteria (Can proceed with documentation)

   **Quality gates can proceed with âš ï¸ NEEDS REVIEW if ONLY warnings**:

   **Code Reviewer Warnings**:
   - ðŸŸ¡ Linting warnings (5-20, documented)
   - ðŸŸ¡ Minor spec gaps (<10% requirements untraceable)
   - ðŸŸ¡ Incomplete documentation (non-blocking)

   **Quality/Tests Reviewer Warnings**:
   - ðŸŸ¡ Coverage 70-75% overall (but critical paths â‰¥90%)
   - ðŸŸ¡ Minor edge cases missing tests
   - ðŸŸ¡ Non-blocking build warnings

   **Security Reviewer Warnings**:
   - ðŸŸ¡ LOW/MEDIUM dependency vulnerabilities (documented)
   - ðŸŸ¡ Partial OWASP coverage (non-critical categories like A09 Logging)
   - ðŸŸ¡ Compliance requirements partially met (document gaps)

   ---

   ### Benefits of Parallel Execution

   1. **Performance**: ~53% faster execution (95s â†’ 45s for typical projects)
   2. **Better User Experience**: All failures shown at once (not iterative "fix â†’ rerun â†’ fail again")
   3. **Maintains Safety**: Still blocks if ANY reviewer fails (âœ… preserves sequential safety guarantees)
   4. **Comprehensive Feedback**: All validation dimensions checked before any blocking
   5. **Time Savings**: Linting (30s), testing (45s), security scans (20s) run simultaneously

   **Example Performance Comparison**:
   - Sequential: 30s (lint) â†’ 45s (test) â†’ 20s (security) = **95s total**
   - Parallel: max(30s, 45s, 20s) = **45s total** â†’ **53% faster**

   ---

10. **Final Validation & Completion**:
   - If Quality Gate status is âŒ NOT READY: HALT and require fixes before proceeding
   - If Quality Gate status is âš ï¸ NEEDS REVIEW: Create backlog tickets, allow commit
   - If Quality Gate status is âœ… READY: Proceed to completion
   - Verify all required tasks are marked [X] in tasks.md
   - Report final status with summary of completed work
   - Suggest running `/speckit.reconcile` for post-implementation gap closure

11. **Implementation Code Review Gate** (Evidence-Based Self-Check):

   **Purpose**: Prevent hallucination and ensure evidence-based completion claims. This gate runs AFTER Step 9 (Parallel Quality Gate Validation) has completed.

   **MANDATORY: The Four Questions** (MUST answer with ACTUAL evidence):

   â“ **"Did all 3 reviewers execute in parallel?"**
      ```yaml
      Action Required:
        - Verify Code, Quality/Tests, and Security reviewers all ran concurrently
        - Show ACTUAL status for each reviewer from parallel execution
        - Report: Aggregated Quality Gate Summary presented

      Expected Evidence:
        Quality Gate Summary:
        âœ“ Code Reviewer: [âœ…/âš ï¸/âŒ] - [N] errors, [M] warnings
        âœ“ Quality/Tests: [âœ…/âš ï¸/âŒ] - [N] failures, [M] warnings
        âœ“ Security: [âœ…/âš ï¸/âŒ] - [N] vulnerabilities, [M] low-severity
        Overall Status: [âœ… READY / âš ï¸ NEEDS REVIEW / âŒ NOT READY]
        Execution Time: [N]s (parallel) vs ~95s (sequential) â†’ [X]% faster

      Hallucination Detection:
        ðŸš¨ "All reviewers passed" WITHOUT showing aggregated report â†’ âŒ BLOCK completion
        ðŸš¨ Claiming parallel execution without timing evidence â†’ âŒ BLOCK completion
        ðŸš¨ Hiding failures from any reviewer â†’ âŒ BLOCK completion
      ```

   â“ **"Are all tests passing?"**
      ```yaml
      Action Required:
        - Run actual test command from plan.md or project configuration
        - Show REAL output (not "probably works" or "should pass")
        - IF any tests fail: Implementation is NOT complete âŒ

      Expected Evidence Format:
        Test Results:
        âœ“ Unit tests: [X]/[Y] passed
        âœ“ Integration tests: [X]/[Y] passed
        âœ“ E2E tests: [X]/[Y] passed (if applicable)
        âœ“ Coverage: [X]% (target: [Y]%)

      Hallucination Detection:
        ðŸš¨ "Tests pass" WITHOUT showing output â†’ âŒ BLOCK completion
        ðŸš¨ "All green" WITHOUT actual results â†’ âŒ BLOCK completion
        ðŸš¨ Hiding test failures or errors â†’ âŒ BLOCK completion
      ```

   â“ **"Are all requirements implemented?"**
      ```yaml
      Action Required:
        - Compare tasks.md checklist vs actual completion status
        - Map each requirement from spec.md to completed tasks
        - List: âœ… Done, âš ï¸ Partial, âŒ Not started

      Expected Evidence Format:
        Requirements Status:
        âœ… R-001: User login (tasks T-001, T-002, T-003)
        âœ… R-002: Session management (tasks T-004, T-005)
        âœ… R-003: JWT tokens (task T-006)
        [... list all requirements with task mappings ...]

      IF any requirement shows âš ï¸ Partial or âŒ:
        â†’ Implementation is NOT complete
        â†’ Report which requirements remain
      ```

   â“ **"Were any assumptions made without verification?"**
      ```yaml
      Self-Reflection Checklist:
        - [ ] Did I consult official documentation for all libraries/APIs?
        - [ ] Did I verify edge cases are handled?
        - [ ] Did I test error scenarios?
        - [ ] Did I validate against actual API responses (not assumed)?
        - [ ] Did I check for security vulnerabilities?

      IF any assumptions were made:
        â†’ Document them explicitly
        â†’ Verify against official sources
        â†’ Test the assumptions
      ```

   â“ **"Do I have evidence to support completion?"**
      ```yaml
      Required Evidence (ALL must be provided):

      1. Test Results (MANDATORY):
         Run: [test command]
         Output: [paste actual output, not summary]

      2. Code Changes (MANDATORY):
         Run: git diff --stat [main-branch]..HEAD
         Output:
           Files modified: [N]
           Lines added: +[N]
           Lines removed: -[N]
           [file list with changes]

      3. Code Quality Validation (MANDATORY):
         Lint:
           Run: [lint command]
           Result: âœ… passed / âŒ [N] errors

         Type Check (if applicable):
           Run: [typecheck command]
           Result: âœ… passed / âŒ [N] errors

         Build:
           Run: [build command]
           Result: âœ… success / âŒ failed with [error]

      4. Git Status (MANDATORY):
         Run: git status
         Output: [show current status]

      IF any evidence is MISSING:
        âŒ CANNOT report completion
        â†’ Gather missing evidence first
        â†’ Re-run this step with complete evidence
      ```

   **Hallucination Prevention (7 Red Flags):**
   ```yaml
   Detect and BLOCK these patterns:

   ðŸš¨ "Tests pass" WITHOUT showing actual test output
      â†’ Self-correction: "Wait, I need to run tests and show real results"

   ðŸš¨ "Everything works" WITHOUT evidence
      â†’ Self-correction: "Let me gather actual evidence"

   ðŸš¨ "Implementation complete" WITH failing tests
      â†’ Self-correction: "Tests are failing, not complete yet"

   ðŸš¨ Skipping error messages
      â†’ Self-correction: "I need to address these errors first"

   ðŸš¨ Ignoring warnings
      â†’ Self-correction: "Let me review and fix warnings"

   ðŸš¨ Hiding failures
      â†’ Self-correction: "I must report failures honestly"

   ðŸš¨ "Probably works" statements
      â†’ Self-correction: "Need to verify, not assume"

   IF detected: STOP â†’ Gather evidence â†’ Report honestly
   ```

   **Output Format** (Present to User - ONLY if ALL evidence provided):
   ```markdown
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   âœ… Implementation Complete - Code Review
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

   ## Test Results

   âœ“ Unit tests: [X]/[Y] passed
   âœ“ Integration tests: [X]/[Y] passed
   âœ“ E2E tests: [X]/[Y] passed (if applicable)
   âœ“ Coverage: [X]% (target: [Y]%)

   Test Output:
   ```
   [paste actual test output]
   ```

   ## Code Quality

   âœ“ Lint: [status]
   âœ“ Type Check: [status]
   âœ“ Build: [status]

   ## Code Changes

   Files modified: [N]
   Lines added: +[N]
   Lines removed: -[N]

   ```
   [git diff --stat output]
   ```

   Commits:
   - [commit 1 message] ([N] files)
   - [commit 2 message] ([N] files)
   [... list all commits ...]

   ## Requirements Completed

   âœ… R-001: [Requirement name] (tasks: [T-IDs])
   âœ… R-002: [Requirement name] (tasks: [T-IDs])
   âœ… R-003: [Requirement name] (tasks: [T-IDs])
   [... list all [X]/[Y] requirements ...]

   ## Outstanding Items

   [IF any exist:]
   âš ï¸ [Item 1]: [Description and impact]
   âš ï¸ [Item 2]: [Description and impact]

   [IF none:]
   âœ… None - all tasks complete

   ## Self-Check Results

   âœ… Official documentation consulted
   âœ… Edge cases handled and tested
   âœ… Error scenarios validated
   âœ… Security vulnerabilities checked
   âœ… No untested assumptions

   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   ## Next Steps
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

   1. Review code changes:
      git diff [main-branch]..HEAD

   2. Review commits:
      git log [main-branch]..HEAD --oneline

   3. Merge to main:
      git checkout main && git merge [branch-name]

   4. Deploy (if applicable):
      [deployment command from plan.md]

   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

   Ready to merge? (yes/review code/run more tests/address issues)
   ```

   **User Interaction:**
   ```yaml
   IF user says "yes" or "merge":
     â†’ Provide git merge commands
     â†’ Suggest deployment steps

   IF user says "review" or "review code":
     â†’ Run: git diff [main-branch]..HEAD
     â†’ Show detailed code changes

   IF user says "more tests" or "run tests":
     â†’ Ask which specific tests to run
     â†’ Execute and show results

   IF user says "address issues":
     â†’ List issues needing attention
     â†’ Wait for fixes, then re-run review gate

   IF EVIDENCE MISSING:
     âŒ "Cannot complete code review without evidence."
     â†’ List missing evidence
     â†’ Gather evidence automatically where possible
     â†’ Re-run review gate
   ```

   **Benefits** (from PM Agent Reflexion pattern):
   - âœ… 94% hallucination detection rate
   - âœ… Evidence-based completion reports
   - âœ… No false "done" claims
   - âœ… Transparent validation process
   - âœ… User confidence in quality

---

## Final Step: Update Specification Metadata

After completing implementation and passing quality gate, update `spec-metadata.json`:

```json
{
  "phase": "implementation",
  "approvals": {
    "implementation": {
      "generated": true,
      "approved": false,
      "timestamp": "{ISO 8601 timestamp}"
    }
  },
  "metadata": {
    "updated_at": "{ISO 8601 timestamp}"
  }
}
```

**Recommend Next Steps**:
- IF quality gate passed: "âœ… Implementation complete. Run `/speckit.reconcile` to verify all requirements met"
- IF quality gate had issues: "âš ï¸ Address [N] issues before marking complete"
- "Or run `/speckit.status` to check current workflow state"

---

Note: This command assumes a complete task breakdown exists in tasks.md. If tasks are incomplete or missing, suggest running `/speckit.tasks` first to regenerate the task list.

